{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Logistic Regression\n",
    "Our goal for this project was to use logistic regression to predict whether a reddit comment will be 'controversial' or not. We took a variety of a different approaches to solving this issue before settling on a model that we deemed best. Our process went as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Approach:  Word frequencies only\n",
    "Our initial approach was to look solely at word counts. Our first thought was to look at just the most frequent words, but we quickly realized that would be cluttered with common words that didn't tell us anything about the body of the comment itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model constructed using entire dataset.\n",
    "\n",
    "We read in the entire CSV and constructed the model from it, and performed logistic regression to produce a model. However, because of the large non-controversial:controversial ratio, the model selected just predicted all x̄ as non-controversial. \n",
    "\n",
    "### Next model used a small subset of the non-controversial data.\n",
    "Changed it to about a 1:1 ratio of controversial to non-controversial, using all of the controversial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Tf-idf Vectorizer\n",
    "We moved onto the tf-idf vectorizer, and picked the top 500 words from both controversial and non-controversial comments by that metric, and tried to fit a model using these counts. However, this model was generating just predictions of non-controversial for every comment, so we quickly moved on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Up: More TF-IDF Vectorization with Unigrams and Bigrams\n",
    "We then began to work on unigram and bigram approach; looking at single words and double words. Using compressed matrices, we were able to get a far more complete picture of our comment bodies. These models began to yield us results that were far more accurate, and actually netted us accuracy beyond picking all as non-controversial. A combination of the top 90% unigram and bigrams was able to get 96% accuracy on our training data, but the threat of overfitting as we saw worse results in our testing data led to us cutting down the number to top 50,000 unigrams/bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![functions](functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](result2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our final Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![finalmodel](finalModel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part II: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classifier\n",
    "Our classifier loads three Pickle models, 'tfidftop50K.pkl', 'tfidfmixbigram.pkl', 'tfidfselector.pkl',\n",
    "which contain the tfidf model, vectorizer, and selector. It stores the beta coefficients in memory to be updated.\n",
    "\n",
    "### Update\n",
    "Update takes a file with new data and, for each x̄, constructs the tfidf and bigram vectors from the data point and uses that to lookup the beta coefficients for the prediction. The product of the beta and tfidf value is used to make the prediction, and we update the betas by increasing them by the residual error over the total number of data points n. This moves the gradient in the direction of minimized residual error (loss).\n",
    "\n",
    "### Predict\n",
    "Predict, similar to Update(), constructs the tfidf vectors from the body of data point x̄, aggregates the product of betas with their tfidf values, and uses the logistic regression equation 1 / (1 + e^(-aggregate)) to pull the resulting value between 0 and 1. If the prediction is less than 0.5, we choose class 0 (non-controversial), otherwise 1 (controversial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
