{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Logistic Regression\n",
    "Our goal for this project was to use logistic regression to predict whether a reddit comment will be 'controversial' or not. We took a variety of a different approaches to solving this issue before settling on a model that we deemed best. Our process went as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Approach:  Word counts only\n",
    "Our initial approach was to look solely at word counts. Our first thought was to look at just the most frequent words, but we quickly realized that would be cluttered with common words that didn't tell us anything about the body of the comment itself. So we moved onto the tf-idf vectorizer, and picked the top 500 words from both controversial and non-controversial comments by that metric, and tried to fit a model using these counts. However, this model was generating just predictions of non-controversial for every comment, so we quickly moved on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Please talk about what happened when we fed in the entire CSV, our model decided to guess everything as non-controversial, and what we decided to do.\n",
    "# Read in all controversial, but an almost 1:1 ratio of controversial to non-controversial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Up: More TF-IDF Vectorization\n",
    "We then began to work on unigram and bigram appraoch; looking at single words and double words. Using compressed matrices, we were able to get a far more complete picture of our comment bodies. These models began to yield us results that were far more accurate, and actually netted us accuracy beyond picking all as non-controversial. A combination of the top 90% unigram and bigrams was able to get 96% accuracy on our training data, but the threat of overfitting as we saw worse results in our testing data led to us cutting down the number to top 50,000 unigrams/bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some points on overfitting\n",
    "### how we saw that if we gave it 90% of data, we would get amazing results with the training set, but when we perform a second time validation, we saw the performance dramatically decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![functions](functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](result2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what we decided to go with, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![finalmodel](finalModel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
